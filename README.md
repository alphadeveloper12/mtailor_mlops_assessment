# MTailor ML Ops Assessment - Image Classification Deployment on Cerebrium

This repository contains all the necessary components to convert a PyTorch classification model to ONNX, test it locally, deploy it using Cerebriumâ€™s serverless GPU platform, and interact with it through API calls.

---

## ğŸš€ Project Structure

```
mtailor_mlops_assessment/
â”‚
â”œâ”€â”€ accessment/                  # Folder for Cerebrium deployment
â”‚   â”œâ”€â”€ cerebrium.toml           # Cerebrium config file
â”‚   â”œâ”€â”€ Dockerfile               # Custom Docker image for deployment
â”‚   â”œâ”€â”€ main.py                  # Main FastAPI app for ONNX model inference
â”‚   â”œâ”€â”€ model.onnx               # Converted ONNX model
â”‚   â”œâ”€â”€ model.py                 # Contains inference and preprocessing logic
â”‚   â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚
â”œâ”€â”€ convert_to_onnx.py           # Script to convert PyTorch model to ONNX
â”œâ”€â”€ model.py                     # Modularized model code (ONNX runtime + preprocessing)
â”œâ”€â”€ pytorch_model.py             # Original PyTorch model definition and weights usage
â”œâ”€â”€ test.py                      # Local test script for inference using ONNX model
â”œâ”€â”€ test_server.py               # Script to test deployed Cerebrium API
â”œâ”€â”€ n01440764_tench.jpeg         # Sample image - class id 0
â”œâ”€â”€ n01667114_mud_turtle.JPEG    # Sample image - class id 35
â”œâ”€â”€ model.onnx                   # ONNX model (top-level copy)
â”œâ”€â”€ resnet18-f37072fd.pth        # PyTorch weights
â””â”€â”€ README.md                    # This file
```


## ğŸ“Œ Requirements

- Python 3.8+
- [Cerebrium CLI](https://docs.cerebrium.ai/)
- Docker
- Git

Create a virtual environment:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r accessment/requirements.txt
```


# ğŸ”§ Step-by-Step Instructions

## âœ… 1. Convert PyTorch to ONNX

```bash
python convert_to_onnx.py
 ```
 ## âœ… 2. Local Testing
 Run the test.py file:

```bash
python test.py --image_path n01440764_tench.jpeg
```
Expected output:
Class ID: 0

## âœ… 3. Deploy on Cerebrium
Navigate to the accessment/ folder:
```bash
cd accessment
```
Login to Cerebrium and initialize project:
```bash
cerebrium login
cerebrium init
```
Then deploy:
```bash
cerebrium deploy
```
## âœ… 4. Test Deployed API
```bash
python test_server.py --image_path ../n01440764_tench.jpeg
```
Expected output:
```bash
{
    "my_result": {
        "class_id": 35
    }
}
```
You can also run internal tests using the --run_tests flag:
```bash
python test_server.py --run_tests --api_key <YOUR_API_KEY> --api_url <MODEL_API_URL>
```
# ğŸ” Files Overview
## convert_to_onnx.py
- Loads the PyTorch model.
- Converts the model to ONNX format.
- Ensures dynamic input shape.
- Applies proper export settings.

##model.py
Contains:

- ImagePreprocessor class: Handles image resizing, normalization.
- OnnxModelHandler class: Loads ONNX model and performs inference.

## main.py
- FastAPI app.
- Exposes /predict endpoint that accepts base64-encoded image and returns class ID.

##test.py
- Tests the model locally using model.onnx.

## test_server.py
- Sends image to the deployed Cerebrium endpoint.
- Has a --run_tests flag to test platform-level behavior.

# ğŸ§ª Testing
- **Unit Tests**: In `test.py`
- **API Tests**: In `test_server.py` with live Cerebrium model
- Supports expected image classification outputs
- Validates:
  - Output class
  - Latency (<3s)

# ğŸ” Future Improvements
- Add GitHub Actions CI for Docker build testing
- Integrate ONNX pre-processing into the model graph itself for faster inference
- Use TorchScript instead of ONNX if needed for speed

# âœ… Submission Checklist
- ONNX conversion
- Local model testing
- FastAPI server with ONNX inference
- Dockerfile for deployment
- Cerebrium deployment with `cerebrium.toml`
- `test_server.py` with live API calls
- `README` with full steps
- Loom walkthrough recorded





